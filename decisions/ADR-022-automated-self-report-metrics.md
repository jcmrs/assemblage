# ADR-022: Automated Self-Report Metrics

**Date:** 2025-11-22
**Status:** Accepted

---

### Ownership & Concurrence

**Decision Domain:** **"How" (Platform)**
**Vision Owner Concurrence (Human Partner):** Yes
**System Owner Concurrence (AI Partner):** Yes

---

## 1. Context

The `feedback` command currently generates an "AI Self-Report" that includes basic git statistics. However, the `self_correction_loops` field is hardcoded to "N/A", and other metrics that could reflect the System Owner's performance (e.g., number of linting errors fixed, test coverage changes) are not captured. To provide a more objective and comprehensive self-assessment, the AI Self-Report needs to incorporate automated, measurable metrics of the System Owner's actions and impact.

## 2. Decision

We will enhance the AI Self-Report generated by the `feedback` command to include **Automated Self-Report Metrics**.

This will involve:

1.  **Refactoring `commit_wrapper.py`:**
    *   Modify the `commit_wrapper` to capture and log structured data about its operations (e.g., number of linting errors before/after, number of auto-fixes applied, test coverage before/after if `validate --coverage` was run).
    *   This data will be stored in a temporary, structured format (e.g., a JSON file) that can be easily consumed by the `feedback` command.
2.  **Enhancing `feedback.py`:**
    *   Modify the `feedback` command to read this structured log data generated by the `commit_wrapper` for the specified commit hash.
    *   Populate the `ai_self_report` section of the feedback YAML with these objective metrics, replacing the "N/A" for `self_correction_loops` and adding new fields as appropriate.
3.  **Integrating with `validate` command:** If the `validate --coverage` command is run as part of the commit process, its output (e.g., percentage coverage) should also be captured and included in the self-report.

## 3. Rationale

*   **Objective Self-Assessment:** Provides concrete, measurable data on the System Owner's performance, moving beyond subjective observations.
*   **Improved Feedback Quality:** Enriches the feedback loop with actionable metrics, allowing for more precise analysis and targeted improvements.
*   **Transparency:** Offers greater insight into the System Owner's internal operations and impact on the codebase.
*   **Foundation for Learning:** These metrics can be used by the `analyze_feedback` command (`ADR-014`) to correlate performance with specific actions or nudges.

## 4. Consequences

- Requires significant modifications to `assemblage/commit_wrapper.py` to capture and store structured data.
- Requires modifications to `assemblage/commands/feedback.py` to read and integrate this data.
- Introduces a new temporary file format for commit-specific metrics.
- The `validate` command's output might need to be parsed more robustly.
